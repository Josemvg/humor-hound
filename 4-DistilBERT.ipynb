{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4 - DistilBERT**\n",
    "\n",
    "# Introduction\n",
    "\n",
    "DistilBERT is a smaller and faster version of the pre-trained BERT (Bidirectional Encoder Representations from Transformers) model, which was introduced by Google in 2018. DistilBERT is designed to be lighter and less resource-intensive than BERT, making it easier to deploy in production environments with limited computational resources.\n",
    "\n",
    "The DistilBERT architecture is based on the same transformer-based architecture as BERT. The transformer-based architecture is composed of a stack of identical transformer blocks, which are used to process sequences of tokens. In BERT, the transformer blocks have 12 layers, with a total of 110 million parameters. In contrast, DistilBERT has only 6 transformer layers, with a total of 66 million parameters. This reduction in parameters is achieved through a process called knowledge distillation, which involves training DistilBERT to mimic the behavior of the larger BERT model.\n",
    "\n",
    "The knowledge distillation process involves training DistilBERT on the same task and dataset as BERT, while using BERT as a teacher model to guide the learning process. During training, the output of the teacher model (BERT) is used to create a soft label for each training example, which is then used to train the student model (DistilBERT). This process allows the student model to learn from the teacher model's outputs, rather than from the ground truth labels, which helps to improve its performance.\n",
    "\n",
    "In addition to reducing the number of parameters, DistilBERT also uses a technique called token-level distillation, which involves simplifying the word embeddings used by the model. In BERT, the word embeddings are learned jointly with the model during training, which can make them quite complex. In contrast, DistilBERT uses a simpler method of generating the word embeddings, which is based on pre-trained word vectors that are learned separately from the model.\n",
    "\n",
    "**Source**: Sanh, V., Debut, L., Chaumond, J., &#38; Wolf, T. (n.d.). <i>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</i>. Retrieved May 7, 2023, from https://github.com/huggingface/transformers\n",
    "\n",
    "In this notebook, we will asses the capabilities of fine-tuning a distilBERT model through the use of two different test cases:\n",
    "\n",
    "1. In the first test-case, we will be using the preprocessed data derived from `0-EDA.ipynb`.\n",
    "2. I the second test, we will be training the model on data that has not being processed. The reason for this is that transformer-based models are targeted towards use in natural-language contexts, in which the user is expected to interact naturally with the model, so we hypothesize that using the preprocessed data as input to the model may end up being counter-productive. \n",
    "\n",
    "\n",
    "For each of the above-described test-cases we will compare performance on both training and testing partitions of the original dataset, as well as performance of the model on *out-of sample* (OSS) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\no-estruc-nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification, TFDistilBertForSequenceClassification, TFTrainingArguments, TFTrainer\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from src.data_exploration.data_preprocessing import train_test_split, tokenization\n",
    "from src.utils.eval_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - With data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first test, we will use the preprocessed data generated in the `0-EDA.ipynb`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientist unveil doomsday clock hair loss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dem rep totally nail congress falling short ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat different recipe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather prevents liar getting work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mother come pretty close using word streaming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  label\n",
       "0          scientist unveil doomsday clock hair loss      1\n",
       "1  dem rep totally nail congress falling short ge...      0\n",
       "2                               eat different recipe      0\n",
       "3                 weather prevents liar getting work      1\n",
       "4  mother come pretty close using word streaming ...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = os.path.join(\"data\", \"Sarcasm_Headlines_Dataset_v2.csv\")\n",
    "\n",
    "df = pd.read_csv(dataset, sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n",
    "\n",
    "We need to generate both training and test sets. We will use 80% of the data for training and the remaining 20% for testing. Moreover, in classification tasks it is important to maintain the same proportion of classes in both training and test sets (otherwise, the model might be affected by it during the training process, and the validation metrics may also be distorted). `scikit-learn` offers the class `StratifiedShuffleSplit` for achieving this, which will be used. We have developed the `train_test_split` function, which is stored in `src/data_preprocessing.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(df, \"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, Padding and Sequencing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with the previous models, in which we used our own tokenizer, in this case we will be using the specific tokenizer that is recommended to achieve peak model performance, which is included in the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train and test sets\n",
    "X_train_tokenized = tokenizer(X_train[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "X_test_tokenized = tokenizer(X_test[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_train_tokenized[\"input_ids\"],\n",
    "    y_train\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_test_tokenized[\"input_ids\"],\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are not training the model from scratch, but rather performing *transfer learning* upon its base to allow us to fine-tune the model to our specific use case. Thus, instead of building the transformer structure of the model from the bottom up, we load the pretrained model using `TFDistilBertForSequenceClassification` and pass the appropriate training arguments, in which we will specify how the model is to be fine-tuned. Both the model structure as well as the training arguments are passed to a `TFTrainer`, who is in charge of computing the fine-tuning process on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_steps = 10,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\venvs\\no-estruc-nlp\\lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the trainer is defined as previously explain, we may proceed with the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7847300809972426}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See loss\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation set\n",
    "output = tf.argmax(trainer.predict(test_dataset)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1538, 1261],\n",
       "       [1448, 1154]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, output)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.53      2799\n",
      "           1       0.48      0.44      0.46      2602\n",
      "\n",
      "    accuracy                           0.50      5401\n",
      "   macro avg       0.50      0.50      0.50      5401\n",
      "weighted avg       0.50      0.50      0.50      5401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the results from this model are not as good as they may seem. The rate of both False Positives and False Negatives is quite high compared to the previously observed models, with levels of both precision and recall that prove that the model is not much better than random guessing. W hypothesize that the cause for this phenomenon is the fact that the data was preprocessed and thus did not fit the criteria for considering it a valid model input.\n",
    "\n",
    "Nonetheless, we can store the model for future use on evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"distilbert_model\")\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on OOS-Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the performance of our trained model in a higher amount of data, we included some web-scrapped news that we manually labelled as being either sarcastic or not. Our goal is to test if the model is performing poorly when presented with real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nextdoor ceo recruit army fanatic holy crusade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exclusive interview clarence thomas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pro con banning book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>know train derailment toxic chemical ohio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cia criticized use abusive etiquette coach bla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  label\n",
       "0  nextdoor ceo recruit army fanatic holy crusade...      1\n",
       "1                exclusive interview clarence thomas      1\n",
       "2                               pro con banning book      1\n",
       "3          know train derailment toxic chemical ohio      1\n",
       "4  cia criticized use abusive etiquette coach bla...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oos_data = pd.read_csv(os.path.join(\"data\", \"Sarcasm_Headlines_Dataset_OOS_Prep.csv\"), sep=\";\", quoting=csv.QUOTE_ALL)\n",
    "oos_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new headlines\n",
    "headlines_tokenized = tokenizer(oos_data[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "headlines_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    headlines_tokenized[\"input_ids\"],\n",
    "    oos_data[\"label\"]\n",
    "))\n",
    "\n",
    "predictions = tf.argmax(trainer.predict(headlines_dataset)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169 311]\n",
      " [226 539]]\n"
     ]
    }
   ],
   "source": [
    "# Get the confussion matrix\n",
    "cm = confusion_matrix(oos_data[\"label\"], predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.35      0.39       480\n",
      "           1       0.63      0.70      0.67       765\n",
      "\n",
      "    accuracy                           0.57      1245\n",
      "   macro avg       0.53      0.53      0.53      1245\n",
      "weighted avg       0.55      0.57      0.56      1245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(oos_data[\"label\"], predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, the model is susprisingly slightly better at predicting OOS data that it was at predicting evaluation data, which could again be due to the format in which the data is presented to the model, as in this case the OOS data is not preprocessed in anyway. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Without data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously stated, we want to assess whether the process of preprocessing data is negatively affecting the ability of the model to adapt to this new classification task, so we will perform another training round, in this case without much preprocessing of data. Of note, we decided to capitalize each of the words in the headlines to avoid loss of important information corresponding to names of people or places. This is due to the fact that the headlines in the original dataset are lower-cased. This causes words such as *Trump* to be converted into *trump*, which can be interpreted as a verb rather than a person, and thus impair the ability of the model to extract context information that may prove to be important for predicting whether a news' headline is sarcastic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thirtysomething Scientists Unveil Doomsday Clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dem Rep. Totally Nails Why Congress Is Falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eat Your Veggies: 9 Deliciously Different Recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inclement Weather Prevents Liar From Getting T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mother Comes Pretty Close To Using Word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  Thirtysomething Scientists Unveil Doomsday Clo...             1\n",
       "1  Dem Rep. Totally Nails Why Congress Is Falling...             0\n",
       "2  Eat Your Veggies: 9 Deliciously Different Recipes             0\n",
       "3  Inclement Weather Prevents Liar From Getting T...             1\n",
       "4  Mother Comes Pretty Close To Using Word 'strea...             1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noprep = pd.read_json('./data/Sarcasm_Headlines_Dataset_v2.json', lines=True)[['headline', 'is_sarcastic']]\n",
    "# capitalize every word in the headline\n",
    "df_noprep['headline'] = df_noprep['headline'].apply(lambda x: ' '.join([word.capitalize() for word in x.split()]))\n",
    "df_noprep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(df_noprep, \"is_sarcastic\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, Padding and Sequencing\n",
    "\n",
    "As mentioned previously, we need to tokenize the text before feeding the data into our model. We will use the pre-trained DistilBERT tokenizer from the transformers class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train and test sets\n",
    "X_train_tokenized = tokenizer(X_train[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "X_test_tokenized = tokenizer(X_test[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_train_tokenized[\"input_ids\"],\n",
    "    y_train\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_test_tokenized[\"input_ids\"],\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_steps = 10,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_39', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\venvs\\no-estruc-nlp\\lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7140190972222222}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See loss\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation set\n",
    "output = tf.argmax(trainer.predict(test_dataset)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1320, 1677],\n",
       "       [ 856, 1871]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, output)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.44      0.51      2997\n",
      "           1       0.53      0.69      0.60      2727\n",
      "\n",
      "    accuracy                           0.56      5724\n",
      "   macro avg       0.57      0.56      0.55      5724\n",
      "weighted avg       0.57      0.56      0.55      5724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, the result in this test case are better than the ones obtained from the previous test case, with a significant decrease in the number of False Negatives, as ewll as a general increase in accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"distilbert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on OOS-Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we aim to assess model performance on OOS data. We hope that in this case the better training process will allow us to obtain better results in the OOS dataset when compared to the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at models\\distilbert_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at models\\distilbert_model and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the path\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "      <th>news_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nextdoor CEO Recruits Army Of Fanatics For Hol...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exclusive Interview With Clarence Thomas</td>\n",
       "      <td>1</td>\n",
       "      <td>The Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pros And Cons Of Banning Books</td>\n",
       "      <td>1</td>\n",
       "      <td>The Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What To Know About The Train Derailment And To...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What To Know About ChatGPT</td>\n",
       "      <td>1</td>\n",
       "      <td>The Onion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  label news_source\n",
       "0  Nextdoor CEO Recruits Army Of Fanatics For Hol...      1   The Onion\n",
       "1           Exclusive Interview With Clarence Thomas      1   The Onion\n",
       "2                     Pros And Cons Of Banning Books      1   The Onion\n",
       "3  What To Know About The Train Derailment And To...      1   The Onion\n",
       "4                         What To Know About ChatGPT      1   The Onion"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oos_data = pd.read_csv(os.path.join(\"data\", \"Sarcasm_Headlines_Dataset_OOS.csv\"), sep=\";\", quoting=csv.QUOTE_ALL)\n",
    "oos_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378/1378 [==============================] - 96s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on new headlines\n",
    "headlines_tokenized = tokenizer(oos_data[\"headline\"].tolist(), truncation=True, padding=True)\n",
    "headlines_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    headlines_tokenized[\"input_ids\"],\n",
    "    oos_data[\"label\"]\n",
    "))\n",
    "\n",
    "predictions = tf.argmax(model.predict(headlines_dataset)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[259 349]\n",
      " [107 663]]\n"
     ]
    }
   ],
   "source": [
    "# Get the confussion matrix\n",
    "cm = confusion_matrix(oos_data[\"label\"], predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.43      0.53       608\n",
      "           1       0.66      0.86      0.74       770\n",
      "\n",
      "    accuracy                           0.67      1378\n",
      "   macro avg       0.68      0.64      0.64      1378\n",
      "weighted avg       0.68      0.67      0.65      1378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(oos_data[\"label\"], predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in this case the results obtained for the OOS batch of data are much better than what we observed for the previous iteration of the model, with an increase in precision, recall, F1 score and overall accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "50eb83a13a54b6c08f9e0958ffb828fbcd82eb99687cd3fc79fb79c1b2ab88e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
